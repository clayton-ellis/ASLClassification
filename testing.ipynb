{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make syre these libraries are installed: torch, torchvision, tqdm, matplotlib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.io import read_image\n",
    "from tqdm import * # Progress Bars\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These are just other files\n",
    "from utils import *\n",
    "from models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 188.82it/s]\n"
     ]
    }
   ],
   "source": [
    "device = 'mps'  \n",
    "# For device, use 'cpu' to run on your cpu, 'mps' runs on the macbook graphics card, if you have cuda, you can also do 'cuda' and it will be very fast\n",
    "\n",
    "data = ASL_Dataset(transform=None, img_shape=64)  # For now, keep img_shape = 64, I havent had time to deal with the model handling other shapes.  \n",
    "                                                  # This is fine for now we can change it later if needed.\n",
    "                                                  # For transform, this is for any of the RandomRotation, etc. from torchvision\n",
    "model = ASL_Classifier(dropout_rate=0.25, device = device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 62400, Validation Size: 3900, Test Size: 11700\n"
     ]
    }
   ],
   "source": [
    "train_proportion = 0.8 # Should definitely be much higher, just wanted to see with small data\n",
    "val_proportion = 0.05\n",
    "test_proportion = 1 - train_proportion - val_proportion\n",
    "\n",
    "n_train = round(len(data) * train_proportion)\n",
    "n_val = round(len(data) * val_proportion)\n",
    "n_test = len(data) - n_train - n_val\n",
    "\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(data, [n_train, n_val, n_test],\n",
    "                                                                torch.Generator().manual_seed(1))\n",
    "print(f'Train Size: {n_train}, Validation Size: {n_val}, Test Size: {n_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/975 [00:00<?, ?it/s]/Users/claytonellis/Desktop/UCSB/Schoolwork/PSTAT 234/ASL Detection/utils.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(read_image(img_path),dtype=torch.float32)\n",
      "/opt/anaconda3/envs/project_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1740: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "100%|██████████| 975/975 [01:07<00:00, 14.48it/s]\n",
      "100%|██████████| 16/16 [00:02<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 3.1560678954002186\n",
      "Epoch 1: Validation Loss = 3.0425856604942907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 975/975 [01:06<00:00, 14.66it/s]\n",
      "100%|██████████| 16/16 [00:02<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 2.9758666926163895\n",
      "Epoch 2: Validation Loss = 2.9089427654559796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 975/975 [01:03<00:00, 15.47it/s]\n",
      "100%|██████████| 16/16 [00:02<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 2.8693943903996395\n",
      "Epoch 3: Validation Loss = 2.8324422865647536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 975/975 [01:03<00:00, 15.35it/s]\n",
      "100%|██████████| 16/16 [00:02<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 2.8138218576480183\n",
      "Epoch 4: Validation Loss = 2.7839718940930487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 975/975 [01:05<00:00, 14.94it/s]\n",
      "100%|██████████| 16/16 [00:02<00:00,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 2.7761076912513145\n",
      "Epoch 5: Validation Loss = 2.752157221084986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=256, shuffle=True)\n",
    "model.train()\n",
    "model.fit(train_loader, val_loader, epochs = 5, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claytonellis/Desktop/UCSB/Schoolwork/PSTAT 234/ASL Detection/utils.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(read_image(img_path),dtype=torch.float32)\n",
      "/opt/anaconda3/envs/project_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1740: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "test_loader = DataLoader(test_data, 64, True)\n",
    "for inputs, outputs in test_loader:\n",
    "    preds = model.predict(inputs.to(device))\n",
    "    y_test = outputs.argmax(dim=2).squeeze()\n",
    "    \n",
    "    all_preds.append(preds)\n",
    "    all_labels.append(y_test)\n",
    "\n",
    "all_preds = torch.cat(all_preds).to('cpu')\n",
    "all_labels = torch.cat(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5573)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy.  The model is naive and barely trained, this isnt surprising\n",
    "(all_preds == all_labels).sum() / all_preds.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
